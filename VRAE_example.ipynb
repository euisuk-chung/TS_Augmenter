{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Contents\n",
    "\n",
    "0. [Load data and preprocess](#Load-data-and-preprocess)\n",
    "1. [Initialize VRAE object](#Initialize-VRAE-object)\n",
    "2. [Fit the model onto dataset](#Fit-the-model-onto-dataset)\n",
    "3. [Transform the input timeseries to encoded latent vectors](#Transform-the-input-timeseries-to-encoded-latent-vectors)\n",
    "4. [Save the model to be fetched later](#Save-the-model-to-be-fetched-later)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from model.vrae import VRAE\n",
    "from model.org_vrae import VRAE\n",
    "\n",
    "from model.utils import *\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm.notebook import trange\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dload = './saved_model' #download directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data and preprocess\n",
    "- `folder` : data location\n",
    "- `cols_to_remove` : generation 수행하지 않을 column 제거"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO : 해당 변수에 대한 처리를 어떻게 해줘야하는가 확인 작업이 필요함**\n",
    "\n",
    "~~~\n",
    "YYYYMMDD : 년월일\n",
    "HHMMSS : 시분초\n",
    "MNG_NO : 장비번호\n",
    "IF_IDX : 회선 index\n",
    "~~~\n",
    "\n",
    "- 현재는 분석의 편의를 위해 ['YYYYMMDD', 'HHMMSS']만 제거해줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23195128, 56)\n"
     ]
    }
   ],
   "source": [
    "# params\n",
    "folder = 'data'\n",
    "cols_to_remove = ['YYYYMMDD', 'HHMMSS']\n",
    "\n",
    "# load data\n",
    "df_total = load_data(folder, cols_to_remove)\n",
    "\n",
    "# shape\n",
    "print(df_total.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HamonDataset(Dataset):\n",
    "    def __init__(self, data, window, stride):\n",
    "        self.data = torch.Tensor(data)\n",
    "        self.window = window\n",
    " \n",
    "    def __len__(self):\n",
    "        return len(self.data) -  self.window \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        x_index = index*self.window\n",
    "        x = self.data[x_index:x_index+self.window]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_total\n",
    "stride = 10\n",
    "window = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.HamonDataset at 0x7f8662f866d8>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = HamonDataset(data, window, stride)\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 56])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fetch `sequence_length` from dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_length = train_dataset[0].shape[0]\n",
    "sequence_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fetch `number_of_features` from dataset**\n",
    "\n",
    "This config corresponds to number of input features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_of_features = train_dataset[0].shape[1]\n",
    "number_of_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 1\n",
    "hidden_size = 90\n",
    "hidden_layer_depth = 1\n",
    "latent_length = 20\n",
    "batch_size = 32\n",
    "learning_rate = 0.0005\n",
    "dropout_rate = 0.2\n",
    "optimizer = 'Adam' # options: ADAM, SGD\n",
    "cuda = True # options: True, False\n",
    "print_every=30\n",
    "clip = True # options: True, False\n",
    "max_grad_norm=5\n",
    "loss = 'MSELoss' # options: SmoothL1Loss, MSELoss\n",
    "block = 'LSTM' # options: LSTM, GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset = train_dataset,\n",
    "                          batch_size = batch_size,\n",
    "                          shuffle = False,\n",
    "                          drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[2.7220e+03, 1.2400e+02, 1.8431e+05,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         [2.7220e+03, 1.2400e+02, 3.8349e+05,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         [2.7220e+03, 1.2400e+02, 2.3519e+05,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         ...,\n",
       "         [2.8500e+03, 1.2400e+02, 2.3200e+02,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         [2.8500e+03, 1.2400e+02, 2.4000e+02,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         [2.8500e+03, 1.2400e+02, 2.4000e+02,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00]],\n",
       "\n",
       "        [[2.8500e+03, 1.2400e+02, 2.4000e+02,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         [2.8500e+03, 1.2400e+02, 2.4000e+02,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         [2.8500e+03, 1.2400e+02, 2.4000e+02,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         ...,\n",
       "         [2.8630e+03, 1.2400e+02, 1.8664e+04,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         [2.8630e+03, 1.2400e+02, 1.9056e+04,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         [2.8630e+03, 1.2400e+02, 1.8104e+04,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00]],\n",
       "\n",
       "        [[2.8630e+03, 1.2400e+02, 1.8096e+04,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         [2.8630e+03, 1.2400e+02, 1.8640e+04,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         [2.8630e+03, 1.2400e+02, 1.9448e+04,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         ...,\n",
       "         [2.8730e+03, 1.2400e+02, 3.3920e+03,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         [2.8730e+03, 1.2400e+02, 3.4480e+03,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         [2.8730e+03, 1.2400e+02, 3.3840e+03,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[3.7730e+03, 1.2400e+02, 2.0880e+03,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         [3.7730e+03, 1.2400e+02, 1.9360e+03,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         [3.7730e+03, 1.2400e+02, 1.9840e+03,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         ...,\n",
       "         [3.7810e+03, 1.2400e+02, 1.7760e+03,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         [3.7810e+03, 1.2400e+02, 1.6800e+03,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         [3.7810e+03, 1.2400e+02, 1.7600e+03,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00]],\n",
       "\n",
       "        [[3.7820e+03, 1.2400e+02, 5.1096e+04,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         [3.7820e+03, 1.2400e+02, 1.2566e+06,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         [3.7820e+03, 1.2400e+02, 5.2016e+04,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         ...,\n",
       "         [3.7900e+03, 1.2400e+02, 1.6496e+04,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         [3.7900e+03, 1.2400e+02, 1.6416e+04,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         [3.7900e+03, 1.2400e+02, 1.6776e+04,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00]],\n",
       "\n",
       "        [[3.7900e+03, 1.2400e+02, 1.6032e+04,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         [3.7900e+03, 1.2400e+02, 1.6528e+04,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         [3.7900e+03, 1.2400e+02, 1.7032e+04,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         ...,\n",
       "         [3.7980e+03, 1.2400e+02, 6.1760e+03,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         [3.7980e+03, 1.2400e+02, 6.1920e+03,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         [3.7980e+03, 1.2400e+02, 5.9840e+03,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00]]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = iter(train_loader).next()\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 100, 56])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize VRAE object\n",
    "\n",
    "VRAE inherits from `sklearn.base.BaseEstimator` and overrides `fit`, `transform` and `fit_transform` functions, similar to sklearn modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "vrae = VRAE(sequence_length=sequence_length,\n",
    "            number_of_features = number_of_features,\n",
    "            hidden_size = hidden_size, \n",
    "            hidden_layer_depth = hidden_layer_depth,\n",
    "            latent_length = latent_length,\n",
    "            batch_size = batch_size,\n",
    "            learning_rate = learning_rate,\n",
    "            n_epochs = n_epochs,\n",
    "            dropout_rate = dropout_rate,\n",
    "            optimizer = optimizer, \n",
    "            cuda = cuda,\n",
    "            print_every=print_every, \n",
    "            clip=clip, \n",
    "            max_grad_norm=max_grad_norm,\n",
    "            loss = loss,\n",
    "            block = block,\n",
    "            dload = dload)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the model onto dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Batch 30, loss = 19379261806739456.0000, recon_loss = 19379261806739456.0000, kl_loss = 0.2303\n",
      "Batch 60, loss = 91916457662742528.0000, recon_loss = 91916457662742528.0000, kl_loss = 0.3164\n",
      "Batch 90, loss = 199138993357455360.0000, recon_loss = 199138993357455360.0000, kl_loss = 0.4391\n",
      "Batch 120, loss = 94359520960053248.0000, recon_loss = 94359520960053248.0000, kl_loss = 0.6790\n",
      "Batch 150, loss = 3174933777612800.0000, recon_loss = 3174933777612800.0000, kl_loss = 0.9729\n",
      "Batch 180, loss = 79180144463314944.0000, recon_loss = 79180144463314944.0000, kl_loss = 1.2229\n",
      "Batch 210, loss = 83308741906137088.0000, recon_loss = 83308741906137088.0000, kl_loss = 1.4732\n",
      "Batch 240, loss = 1843474051629056.0000, recon_loss = 1843474051629056.0000, kl_loss = 1.8257\n",
      "Batch 270, loss = 37535862388424704.0000, recon_loss = 37535862388424704.0000, kl_loss = 1.6207\n",
      "Batch 300, loss = 98044018784468992.0000, recon_loss = 98044018784468992.0000, kl_loss = 2.0940\n",
      "Batch 330, loss = 14806790231293952.0000, recon_loss = 14806790231293952.0000, kl_loss = 2.3544\n",
      "Batch 360, loss = 2522978948481024.0000, recon_loss = 2522978948481024.0000, kl_loss = 1.5472\n",
      "Batch 390, loss = 87295141571723264.0000, recon_loss = 87295141571723264.0000, kl_loss = 2.5580\n",
      "Batch 420, loss = 2241480897058570240.0000, recon_loss = 2241480897058570240.0000, kl_loss = 2.3133\n",
      "Batch 450, loss = 3360706682421248.0000, recon_loss = 3360706682421248.0000, kl_loss = 1.7848\n",
      "Batch 480, loss = 234995098331381760.0000, recon_loss = 234995098331381760.0000, kl_loss = 2.5119\n",
      "Batch 510, loss = 213598636414599168.0000, recon_loss = 213598636414599168.0000, kl_loss = 2.1470\n",
      "Batch 540, loss = 136755057239523328.0000, recon_loss = 136755057239523328.0000, kl_loss = 1.8140\n",
      "Batch 570, loss = 1136399968009977856.0000, recon_loss = 1136399968009977856.0000, kl_loss = 2.7262\n",
      "Batch 600, loss = 41889005016449024.0000, recon_loss = 41889005016449024.0000, kl_loss = 2.9700\n",
      "Batch 630, loss = 93917534465556480.0000, recon_loss = 93917534465556480.0000, kl_loss = 3.0759\n",
      "Batch 660, loss = 1300890618377011200.0000, recon_loss = 1300890618377011200.0000, kl_loss = 2.6904\n",
      "Batch 690, loss = 132702008271437824.0000, recon_loss = 132702008271437824.0000, kl_loss = 3.0196\n",
      "Batch 720, loss = 41139730791792640.0000, recon_loss = 41139730791792640.0000, kl_loss = 3.2321\n",
      "Batch 750, loss = 549135861128626176.0000, recon_loss = 549135861128626176.0000, kl_loss = 3.1368\n",
      "Batch 780, loss = 1613814238982176768.0000, recon_loss = 1613814238982176768.0000, kl_loss = 3.3273\n",
      "Batch 810, loss = 642778414608023552.0000, recon_loss = 642778414608023552.0000, kl_loss = 3.0262\n",
      "Batch 840, loss = 300460312706940928.0000, recon_loss = 300460312706940928.0000, kl_loss = 3.2334\n",
      "Batch 870, loss = 258167391786106880.0000, recon_loss = 258167391786106880.0000, kl_loss = 3.3916\n",
      "Batch 900, loss = 4611922997542912.0000, recon_loss = 4611922997542912.0000, kl_loss = 3.4092\n",
      "Batch 930, loss = 985828035338960896.0000, recon_loss = 985828035338960896.0000, kl_loss = 3.1072\n",
      "Batch 960, loss = 281314774430515200.0000, recon_loss = 281314774430515200.0000, kl_loss = 3.3649\n",
      "Batch 990, loss = 1010036051567181824.0000, recon_loss = 1010036051567181824.0000, kl_loss = 3.5526\n",
      "Batch 1020, loss = 625583358419664896.0000, recon_loss = 625583358419664896.0000, kl_loss = 3.0276\n",
      "Batch 1050, loss = 3345548351264260096.0000, recon_loss = 3345548351264260096.0000, kl_loss = 3.4595\n",
      "Batch 1080, loss = 707813221720915968.0000, recon_loss = 707813221720915968.0000, kl_loss = 3.3172\n",
      "Batch 1110, loss = 448382869031616512.0000, recon_loss = 448382869031616512.0000, kl_loss = 3.5179\n",
      "Batch 1140, loss = 621316703548080128.0000, recon_loss = 621316703548080128.0000, kl_loss = 3.4117\n",
      "Batch 1170, loss = 649576969880469504.0000, recon_loss = 649576969880469504.0000, kl_loss = 3.5944\n",
      "Batch 1200, loss = 262826331790639104.0000, recon_loss = 262826331790639104.0000, kl_loss = 3.6796\n",
      "Batch 1230, loss = 637669877426946048.0000, recon_loss = 637669877426946048.0000, kl_loss = 3.7638\n",
      "Batch 1260, loss = 80988377234538496.0000, recon_loss = 80988377234538496.0000, kl_loss = 3.9598\n",
      "Batch 1290, loss = 168746705259331584.0000, recon_loss = 168746705259331584.0000, kl_loss = 3.7831\n",
      "Batch 1320, loss = 286936147886604288.0000, recon_loss = 286936147886604288.0000, kl_loss = 3.4293\n",
      "Batch 1350, loss = 71298561647575040.0000, recon_loss = 71298561647575040.0000, kl_loss = 3.5952\n",
      "Batch 1380, loss = 876040699793899520.0000, recon_loss = 876040699793899520.0000, kl_loss = 3.2339\n",
      "Batch 1410, loss = 9273483378802294784.0000, recon_loss = 9273483378802294784.0000, kl_loss = 3.2680\n",
      "Batch 1440, loss = 41132426529890893824.0000, recon_loss = 41132426529890893824.0000, kl_loss = 2.6957\n",
      "Batch 1470, loss = 33044045162017193984.0000, recon_loss = 33044045162017193984.0000, kl_loss = 2.8820\n",
      "Batch 1500, loss = 31619603658977574912.0000, recon_loss = 31619603658977574912.0000, kl_loss = 2.7427\n",
      "Batch 1530, loss = 38652803508883947520.0000, recon_loss = 38652803508883947520.0000, kl_loss = 2.7473\n",
      "Batch 1560, loss = 32683682425036865536.0000, recon_loss = 32683682425036865536.0000, kl_loss = 2.7802\n",
      "Batch 1590, loss = 25223335501879050240.0000, recon_loss = 25223335501879050240.0000, kl_loss = 2.8415\n",
      "Batch 1620, loss = 33390400120859656192.0000, recon_loss = 33390400120859656192.0000, kl_loss = 2.8633\n",
      "Batch 1650, loss = 47161906404056367104.0000, recon_loss = 47161906404056367104.0000, kl_loss = 3.1276\n",
      "Batch 1680, loss = 43688496395353849856.0000, recon_loss = 43688496395353849856.0000, kl_loss = 2.9582\n",
      "Batch 1710, loss = 33433166725133631488.0000, recon_loss = 33433166725133631488.0000, kl_loss = 2.9795\n",
      "Batch 1740, loss = 23964753128866185216.0000, recon_loss = 23964753128866185216.0000, kl_loss = 2.9744\n",
      "Batch 1770, loss = 16878229164035932160.0000, recon_loss = 16878229164035932160.0000, kl_loss = 3.0740\n",
      "Batch 1800, loss = 5926930623666061312.0000, recon_loss = 5926930623666061312.0000, kl_loss = 3.0954\n",
      "Batch 1830, loss = 4814176477845127168.0000, recon_loss = 4814176477845127168.0000, kl_loss = 3.2625\n",
      "Batch 1860, loss = 4582059402739056640.0000, recon_loss = 4582059402739056640.0000, kl_loss = 3.4350\n",
      "Batch 1890, loss = 4817600357054021632.0000, recon_loss = 4817600357054021632.0000, kl_loss = 3.7322\n",
      "Batch 1920, loss = 4375878432542687232.0000, recon_loss = 4375878432542687232.0000, kl_loss = 3.8269\n",
      "Batch 1950, loss = 4619853740554321920.0000, recon_loss = 4619853740554321920.0000, kl_loss = 4.0293\n",
      "Batch 1980, loss = 2280669140739948544.0000, recon_loss = 2280669140739948544.0000, kl_loss = 3.8210\n",
      "Batch 2010, loss = 2447821846196781056.0000, recon_loss = 2447821846196781056.0000, kl_loss = 3.5923\n",
      "Batch 2040, loss = 1861221122784100352.0000, recon_loss = 1861221122784100352.0000, kl_loss = 3.5020\n",
      "Batch 2070, loss = 1222147581325344768.0000, recon_loss = 1222147581325344768.0000, kl_loss = 3.5126\n",
      "Batch 2100, loss = 909030515393167360.0000, recon_loss = 909030515393167360.0000, kl_loss = 4.1649\n",
      "Batch 2130, loss = 431430530035089408.0000, recon_loss = 431430530035089408.0000, kl_loss = 4.1230\n",
      "Batch 2160, loss = 543629197659078656.0000, recon_loss = 543629197659078656.0000, kl_loss = 4.0491\n",
      "Batch 2190, loss = 263748701787258880.0000, recon_loss = 263748701787258880.0000, kl_loss = 3.8632\n",
      "Batch 2220, loss = 185978114050883584.0000, recon_loss = 185978114050883584.0000, kl_loss = 4.0508\n",
      "Batch 2250, loss = 114880943419817984.0000, recon_loss = 114880943419817984.0000, kl_loss = 4.1684\n",
      "Batch 2280, loss = 126620695357554688.0000, recon_loss = 126620695357554688.0000, kl_loss = 4.2360\n",
      "Batch 2310, loss = 95448836335468544.0000, recon_loss = 95448836335468544.0000, kl_loss = 3.9150\n",
      "Batch 2340, loss = 333798536053260288.0000, recon_loss = 333798536053260288.0000, kl_loss = 3.7986\n",
      "Batch 2370, loss = 143422951016890368.0000, recon_loss = 143422951016890368.0000, kl_loss = 4.5492\n",
      "Batch 2400, loss = 193812289017610240.0000, recon_loss = 193812289017610240.0000, kl_loss = 4.2353\n",
      "Batch 2430, loss = 474188475654995968.0000, recon_loss = 474188475654995968.0000, kl_loss = 3.9627\n",
      "Batch 2460, loss = 5450960286339039232.0000, recon_loss = 5450960286339039232.0000, kl_loss = 3.4399\n",
      "Batch 2490, loss = 22902910769457790976.0000, recon_loss = 22902910769457790976.0000, kl_loss = 3.1981\n",
      "Batch 2520, loss = 37486828602232143872.0000, recon_loss = 37486828602232143872.0000, kl_loss = 3.2248\n",
      "Batch 2550, loss = 44153347921344987136.0000, recon_loss = 44153347921344987136.0000, kl_loss = 3.1029\n",
      "Batch 2580, loss = 40896519713081786368.0000, recon_loss = 40896519713081786368.0000, kl_loss = 3.2632\n",
      "Batch 2610, loss = 39113375735619780608.0000, recon_loss = 39113375735619780608.0000, kl_loss = 3.1936\n",
      "Batch 2640, loss = 31401695847515160576.0000, recon_loss = 31401695847515160576.0000, kl_loss = 3.1836\n",
      "Batch 2670, loss = 20910998121183444992.0000, recon_loss = 20910998121183444992.0000, kl_loss = 2.9999\n",
      "Batch 2700, loss = 35076782677030862848.0000, recon_loss = 35076782677030862848.0000, kl_loss = 3.1315\n",
      "Batch 2730, loss = 39445318296045355008.0000, recon_loss = 39445318296045355008.0000, kl_loss = 3.1214\n",
      "Batch 2760, loss = 38884008813972684800.0000, recon_loss = 38884008813972684800.0000, kl_loss = 3.1407\n",
      "Batch 2790, loss = 30149294928873652224.0000, recon_loss = 30149294928873652224.0000, kl_loss = 3.1564\n",
      "Batch 2820, loss = 21874143918836154368.0000, recon_loss = 21874143918836154368.0000, kl_loss = 3.5414\n",
      "Batch 2850, loss = 13545975557618925568.0000, recon_loss = 13545975557618925568.0000, kl_loss = 3.5089\n",
      "Batch 2880, loss = 6872026288681385984.0000, recon_loss = 6872026288681385984.0000, kl_loss = 3.6535\n",
      "Batch 2910, loss = 4224240385909587968.0000, recon_loss = 4224240385909587968.0000, kl_loss = 3.8963\n",
      "Batch 2940, loss = 4729494841052889088.0000, recon_loss = 4729494841052889088.0000, kl_loss = 4.1375\n",
      "Batch 2970, loss = 3564959195121844224.0000, recon_loss = 3564959195121844224.0000, kl_loss = 4.2535\n",
      "Batch 3000, loss = 5526420319109120000.0000, recon_loss = 5526420319109120000.0000, kl_loss = 4.2485\n",
      "Batch 3030, loss = 4557787408677994496.0000, recon_loss = 4557787408677994496.0000, kl_loss = 4.1133\n",
      "Batch 3060, loss = 2583350398215520256.0000, recon_loss = 2583350398215520256.0000, kl_loss = 4.1987\n",
      "Batch 3090, loss = 2375521809844928512.0000, recon_loss = 2375521809844928512.0000, kl_loss = 4.2191\n",
      "Batch 3120, loss = 2253456365391446016.0000, recon_loss = 2253456365391446016.0000, kl_loss = 3.7260\n",
      "Batch 3150, loss = 650683422175395840.0000, recon_loss = 650683422175395840.0000, kl_loss = 3.8122\n",
      "Batch 3180, loss = 1173110462238162944.0000, recon_loss = 1173110462238162944.0000, kl_loss = 3.8877\n",
      "Batch 3210, loss = 369838328188502016.0000, recon_loss = 369838328188502016.0000, kl_loss = 4.1504\n",
      "Batch 3240, loss = 467194791428358144.0000, recon_loss = 467194791428358144.0000, kl_loss = 4.5172\n",
      "Batch 3270, loss = 447052322523054080.0000, recon_loss = 447052322523054080.0000, kl_loss = 4.2178\n",
      "Batch 3300, loss = 144394962245517312.0000, recon_loss = 144394962245517312.0000, kl_loss = 4.3927\n",
      "Batch 3330, loss = 306539271978745856.0000, recon_loss = 306539271978745856.0000, kl_loss = 4.4975\n",
      "Batch 3360, loss = 239573550648786944.0000, recon_loss = 239573550648786944.0000, kl_loss = 4.3327\n",
      "Batch 3390, loss = 168214215213973504.0000, recon_loss = 168214215213973504.0000, kl_loss = 4.2009\n",
      "Batch 3420, loss = 100749882540687360.0000, recon_loss = 100749882540687360.0000, kl_loss = 4.0564\n",
      "Batch 3450, loss = 160620403797000192.0000, recon_loss = 160620403797000192.0000, kl_loss = 3.9211\n",
      "Batch 3480, loss = 239976126523375616.0000, recon_loss = 239976126523375616.0000, kl_loss = 3.6577\n",
      "Batch 3510, loss = 5052528608514408448.0000, recon_loss = 5052528608514408448.0000, kl_loss = 3.8262\n",
      "Batch 3540, loss = 26966639775020220416.0000, recon_loss = 26966639775020220416.0000, kl_loss = 3.2408\n",
      "Batch 3570, loss = 51464722014055432192.0000, recon_loss = 51464722014055432192.0000, kl_loss = 3.1268\n",
      "Batch 3600, loss = 39488463132319285248.0000, recon_loss = 39488463132319285248.0000, kl_loss = 3.2909\n",
      "Batch 3630, loss = 41163146884770955264.0000, recon_loss = 41163146884770955264.0000, kl_loss = 3.2126\n",
      "Batch 3660, loss = 36849833137749884928.0000, recon_loss = 36849833137749884928.0000, kl_loss = 3.1998\n",
      "Batch 3690, loss = 36693726675861504000.0000, recon_loss = 36693726675861504000.0000, kl_loss = 3.4663\n",
      "Batch 3720, loss = 27259246207450480640.0000, recon_loss = 27259246207450480640.0000, kl_loss = 3.4073\n",
      "Batch 3750, loss = 43833860628638859264.0000, recon_loss = 43833860628638859264.0000, kl_loss = 3.6197\n",
      "Batch 3780, loss = 29025050886542458880.0000, recon_loss = 29025050886542458880.0000, kl_loss = 3.3380\n",
      "Batch 3810, loss = 29348852662875979776.0000, recon_loss = 29348852662875979776.0000, kl_loss = 3.4324\n",
      "Batch 3840, loss = 29194448244987396096.0000, recon_loss = 29194448244987396096.0000, kl_loss = 3.4032\n",
      "Batch 3870, loss = 23986545449328705536.0000, recon_loss = 23986545449328705536.0000, kl_loss = 3.4741\n",
      "Batch 3900, loss = 11338648790253961216.0000, recon_loss = 11338648790253961216.0000, kl_loss = 3.4304\n",
      "Batch 3930, loss = 10976542227831980032.0000, recon_loss = 10976542227831980032.0000, kl_loss = 3.9526\n",
      "Batch 3960, loss = 4114279877283151872.0000, recon_loss = 4114279877283151872.0000, kl_loss = 4.0535\n",
      "Batch 3990, loss = 5390341461520678912.0000, recon_loss = 5390341461520678912.0000, kl_loss = 4.4559\n",
      "Batch 4020, loss = 3178166947388850176.0000, recon_loss = 3178166947388850176.0000, kl_loss = 4.4672\n",
      "Batch 4050, loss = 3608196390372507648.0000, recon_loss = 3608196390372507648.0000, kl_loss = 4.3946\n",
      "Batch 4080, loss = 2878138811471101952.0000, recon_loss = 2878138811471101952.0000, kl_loss = 3.9146\n",
      "Batch 4110, loss = 1231594035475382272.0000, recon_loss = 1231594035475382272.0000, kl_loss = 3.8930\n",
      "Batch 4140, loss = 716228883719913472.0000, recon_loss = 716228883719913472.0000, kl_loss = 3.9441\n",
      "Batch 4170, loss = 446832901233836032.0000, recon_loss = 446832901233836032.0000, kl_loss = 4.1965\n",
      "Batch 4200, loss = 366228768953466880.0000, recon_loss = 366228768953466880.0000, kl_loss = 4.2730\n",
      "Batch 4230, loss = 703739393701052416.0000, recon_loss = 703739393701052416.0000, kl_loss = 4.0138\n",
      "Batch 4260, loss = 486687414601908224.0000, recon_loss = 486687414601908224.0000, kl_loss = 4.2183\n",
      "Batch 4290, loss = 186581488236494848.0000, recon_loss = 186581488236494848.0000, kl_loss = 4.3824\n",
      "Batch 4320, loss = 44344970395516928.0000, recon_loss = 44344970395516928.0000, kl_loss = 4.1209\n",
      "Batch 4350, loss = 623474495117590528.0000, recon_loss = 623474495117590528.0000, kl_loss = 4.1445\n",
      "Batch 4380, loss = 5871937016299520.0000, recon_loss = 5871937016299520.0000, kl_loss = 4.4229\n",
      "Batch 4410, loss = 20847337463087104.0000, recon_loss = 20847337463087104.0000, kl_loss = 2.6880\n",
      "Batch 4440, loss = 133099404415467520.0000, recon_loss = 133099404415467520.0000, kl_loss = 4.1744\n",
      "Batch 4470, loss = 133374505660710912.0000, recon_loss = 133374505660710912.0000, kl_loss = 4.3157\n",
      "Batch 4500, loss = 540973602200092672.0000, recon_loss = 540973602200092672.0000, kl_loss = 2.9365\n",
      "Batch 4530, loss = 7689827094502572032.0000, recon_loss = 7689827094502572032.0000, kl_loss = 3.9271\n",
      "Batch 4560, loss = 28747166914808119296.0000, recon_loss = 28747166914808119296.0000, kl_loss = 2.9858\n",
      "Batch 4590, loss = 22590240448843874304.0000, recon_loss = 22590240448843874304.0000, kl_loss = 3.0301\n",
      "Batch 4620, loss = 52246100947450724352.0000, recon_loss = 52246100947450724352.0000, kl_loss = 2.5943\n",
      "Batch 4650, loss = 82701148069776326656.0000, recon_loss = 82701148069776326656.0000, kl_loss = 2.7881\n",
      "Batch 4680, loss = 29883054785277460480.0000, recon_loss = 29883054785277460480.0000, kl_loss = 2.6741\n",
      "Batch 4710, loss = 37036961220704337920.0000, recon_loss = 37036961220704337920.0000, kl_loss = 2.5181\n",
      "Batch 4740, loss = 35516464182838951936.0000, recon_loss = 35516464182838951936.0000, kl_loss = 2.7543\n",
      "Batch 4770, loss = 18620068887688904704.0000, recon_loss = 18620068887688904704.0000, kl_loss = 2.4356\n",
      "Batch 4800, loss = 22021608219330215936.0000, recon_loss = 22021608219330215936.0000, kl_loss = 2.6314\n",
      "Batch 4830, loss = 34739850532838440960.0000, recon_loss = 34739850532838440960.0000, kl_loss = 2.4388\n",
      "Batch 4860, loss = 25963619088674586624.0000, recon_loss = 25963619088674586624.0000, kl_loss = 2.7116\n",
      "Batch 4890, loss = 14638418425139953664.0000, recon_loss = 14638418425139953664.0000, kl_loss = 3.6878\n",
      "Batch 4920, loss = 22355164862825365504.0000, recon_loss = 22355164862825365504.0000, kl_loss = 3.1402\n",
      "Batch 4950, loss = 3773681411567910912.0000, recon_loss = 3773681411567910912.0000, kl_loss = 4.2819\n",
      "Batch 4980, loss = 5482600382695735296.0000, recon_loss = 5482600382695735296.0000, kl_loss = 4.0357\n",
      "Batch 5010, loss = 4636795015715094528.0000, recon_loss = 4636795015715094528.0000, kl_loss = 3.9072\n",
      "Batch 5040, loss = 2525967985873518592.0000, recon_loss = 2525967985873518592.0000, kl_loss = 4.3734\n",
      "Batch 5070, loss = 4398653441522532352.0000, recon_loss = 4398653441522532352.0000, kl_loss = 4.0022\n",
      "Batch 5100, loss = 1753061064448147456.0000, recon_loss = 1753061064448147456.0000, kl_loss = 4.0580\n",
      "Batch 5130, loss = 23056926338187264.0000, recon_loss = 23056926338187264.0000, kl_loss = 4.4220\n",
      "Batch 5160, loss = 1363283092889927680.0000, recon_loss = 1363283092889927680.0000, kl_loss = 4.3436\n",
      "Batch 5190, loss = 5498536154472906752.0000, recon_loss = 5498536154472906752.0000, kl_loss = 4.4744\n",
      "Batch 5220, loss = 16755965575561216.0000, recon_loss = 16755965575561216.0000, kl_loss = 4.4271\n",
      "Batch 5250, loss = 1270923841278836736.0000, recon_loss = 1270923841278836736.0000, kl_loss = 4.3068\n",
      "Batch 5280, loss = 442062841835421696.0000, recon_loss = 442062841835421696.0000, kl_loss = 4.0832\n",
      "Batch 5310, loss = 298905190948405248.0000, recon_loss = 298905190948405248.0000, kl_loss = 4.1592\n",
      "Batch 5340, loss = 238996891159756800.0000, recon_loss = 238996891159756800.0000, kl_loss = 4.3411\n",
      "Batch 5370, loss = 247322015248154624.0000, recon_loss = 247322015248154624.0000, kl_loss = 4.3920\n",
      "Batch 5400, loss = 154758649611288576.0000, recon_loss = 154758649611288576.0000, kl_loss = 4.6549\n",
      "Batch 5430, loss = 206606034519719936.0000, recon_loss = 206606034519719936.0000, kl_loss = 4.1851\n",
      "Batch 5460, loss = 167260131178840064.0000, recon_loss = 167260131178840064.0000, kl_loss = 4.6311\n",
      "Batch 5490, loss = 38085407748915200.0000, recon_loss = 38085407748915200.0000, kl_loss = 4.5142\n",
      "Batch 5520, loss = 229590637903609856.0000, recon_loss = 229590637903609856.0000, kl_loss = 4.4933\n",
      "Batch 5550, loss = 598970916419928064.0000, recon_loss = 598970916419928064.0000, kl_loss = 4.5246\n",
      "Batch 5580, loss = 9162174319164391424.0000, recon_loss = 9162174319164391424.0000, kl_loss = 4.0459\n",
      "Batch 5610, loss = 18752768946045190144.0000, recon_loss = 18752768946045190144.0000, kl_loss = 3.1402\n",
      "Batch 5640, loss = 46132710743899897856.0000, recon_loss = 46132710743899897856.0000, kl_loss = 3.4756\n",
      "Batch 5670, loss = 56491148216526438400.0000, recon_loss = 56491148216526438400.0000, kl_loss = 3.8068\n",
      "Batch 5700, loss = 51352796128394346496.0000, recon_loss = 51352796128394346496.0000, kl_loss = 3.7958\n",
      "Batch 5730, loss = 44378198049225179136.0000, recon_loss = 44378198049225179136.0000, kl_loss = 3.7959\n",
      "Batch 5760, loss = 30400687267448356864.0000, recon_loss = 30400687267448356864.0000, kl_loss = 3.3198\n",
      "Batch 5790, loss = 19593459934689230848.0000, recon_loss = 19593459934689230848.0000, kl_loss = 3.3319\n",
      "Batch 5820, loss = 27533295082627137536.0000, recon_loss = 27533295082627137536.0000, kl_loss = 3.4967\n",
      "Batch 5850, loss = 42969991934881300480.0000, recon_loss = 42969991934881300480.0000, kl_loss = 3.5098\n",
      "Batch 5880, loss = 21631044096958136320.0000, recon_loss = 21631044096958136320.0000, kl_loss = 3.8035\n",
      "Batch 5910, loss = 15462190128855777280.0000, recon_loss = 15462190128855777280.0000, kl_loss = 3.7990\n",
      "Batch 5940, loss = 7309429056640909312.0000, recon_loss = 7309429056640909312.0000, kl_loss = 4.2627\n",
      "Batch 5970, loss = 2850995992549916672.0000, recon_loss = 2850995992549916672.0000, kl_loss = 4.1549\n",
      "Batch 6000, loss = 1554018123739299840.0000, recon_loss = 1554018123739299840.0000, kl_loss = 4.4502\n",
      "Batch 6030, loss = 2325770283077599232.0000, recon_loss = 2325770283077599232.0000, kl_loss = 4.4271\n",
      "Batch 6060, loss = 2847587506503811072.0000, recon_loss = 2847587506503811072.0000, kl_loss = 4.0779\n",
      "Batch 6090, loss = 2469935773810425856.0000, recon_loss = 2469935773810425856.0000, kl_loss = 4.1663\n",
      "Batch 6120, loss = 2245705220732485632.0000, recon_loss = 2245705220732485632.0000, kl_loss = 4.1916\n",
      "Batch 6150, loss = 783402309668306944.0000, recon_loss = 783402309668306944.0000, kl_loss = 4.3938\n",
      "Batch 6180, loss = 455495231794577408.0000, recon_loss = 455495231794577408.0000, kl_loss = 4.6480\n",
      "Batch 6210, loss = 1036774044050915328.0000, recon_loss = 1036774044050915328.0000, kl_loss = 4.6459\n",
      "Batch 6240, loss = 411177457131978752.0000, recon_loss = 411177457131978752.0000, kl_loss = 4.3484\n",
      "Batch 6270, loss = 366216227648962560.0000, recon_loss = 366216227648962560.0000, kl_loss = 4.2844\n",
      "Batch 6300, loss = 156521802405773312.0000, recon_loss = 156521802405773312.0000, kl_loss = 4.3027\n",
      "Batch 6330, loss = 262620791835721728.0000, recon_loss = 262620791835721728.0000, kl_loss = 4.4314\n",
      "Batch 6360, loss = 400751681718976512.0000, recon_loss = 400751681718976512.0000, kl_loss = 4.3693\n",
      "Batch 6390, loss = 225757877808136192.0000, recon_loss = 225757877808136192.0000, kl_loss = 4.5254\n",
      "Batch 6420, loss = 120554973174956032.0000, recon_loss = 120554973174956032.0000, kl_loss = 4.0792\n",
      "Batch 6450, loss = 128496402064998400.0000, recon_loss = 128496402064998400.0000, kl_loss = 4.3338\n",
      "Batch 6480, loss = 77239343231533056.0000, recon_loss = 77239343231533056.0000, kl_loss = 4.1209\n",
      "Batch 6510, loss = 37705599495962624.0000, recon_loss = 37705599495962624.0000, kl_loss = 4.5074\n",
      "Batch 6540, loss = 80322141907582976.0000, recon_loss = 80322141907582976.0000, kl_loss = 4.3196\n",
      "Batch 6570, loss = 77846574297776128.0000, recon_loss = 77846574297776128.0000, kl_loss = 4.4622\n",
      "Batch 6600, loss = 775562448164880384.0000, recon_loss = 775562448164880384.0000, kl_loss = 4.3608\n",
      "Batch 6630, loss = 2675430648751063040.0000, recon_loss = 2675430648751063040.0000, kl_loss = 4.4058\n",
      "Batch 6660, loss = 1017110446819246080.0000, recon_loss = 1017110446819246080.0000, kl_loss = 4.5588\n",
      "Batch 6690, loss = 1179096478417682432.0000, recon_loss = 1179096478417682432.0000, kl_loss = 4.0587\n",
      "Batch 6720, loss = 1428724375707713536.0000, recon_loss = 1428724375707713536.0000, kl_loss = 4.2508\n",
      "Batch 6750, loss = 1859639750185451520.0000, recon_loss = 1859639750185451520.0000, kl_loss = 4.0286\n",
      "Batch 6780, loss = 651870070099673088.0000, recon_loss = 651870070099673088.0000, kl_loss = 4.3571\n",
      "Batch 6810, loss = 629231538000625664.0000, recon_loss = 629231538000625664.0000, kl_loss = 4.4887\n",
      "Batch 6840, loss = 1266330494014849024.0000, recon_loss = 1266330494014849024.0000, kl_loss = 4.4959\n",
      "Batch 6870, loss = 1128287908659200000.0000, recon_loss = 1128287908659200000.0000, kl_loss = 4.5587\n",
      "Batch 6900, loss = 1153398005458534400.0000, recon_loss = 1153398005458534400.0000, kl_loss = 4.3432\n",
      "Batch 6930, loss = 461640230483525632.0000, recon_loss = 461640230483525632.0000, kl_loss = 4.5066\n",
      "Batch 6960, loss = 820025186563653632.0000, recon_loss = 820025186563653632.0000, kl_loss = 4.6680\n",
      "Batch 6990, loss = 422697555852263424.0000, recon_loss = 422697555852263424.0000, kl_loss = 4.6864\n",
      "Batch 7020, loss = 373239529970073600.0000, recon_loss = 373239529970073600.0000, kl_loss = 4.6832\n",
      "Batch 7050, loss = 316716489044393984.0000, recon_loss = 316716489044393984.0000, kl_loss = 4.5528\n",
      "Batch 7080, loss = 410488303859531776.0000, recon_loss = 410488303859531776.0000, kl_loss = 3.8536\n",
      "Batch 7110, loss = 261555605586575360.0000, recon_loss = 261555605586575360.0000, kl_loss = 4.6350\n",
      "Batch 7140, loss = 601974782187012096.0000, recon_loss = 601974782187012096.0000, kl_loss = 3.9559\n",
      "Batch 7170, loss = 205541209047826432.0000, recon_loss = 205541209047826432.0000, kl_loss = 3.9270\n",
      "Batch 7200, loss = 219844412216180736.0000, recon_loss = 219844412216180736.0000, kl_loss = 4.4065\n",
      "Batch 7230, loss = 385543855358869504.0000, recon_loss = 385543855358869504.0000, kl_loss = 4.3305\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [100, 56] at entry 0 and [28, 56] at entry 15",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-180b54515e23>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvrae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#If the model has to be saved, with the learnt parameters use:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# vrae.fit(dataset, save = True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/repo/projects/timeseries-generation/model/org_vrae.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, save)\u001b[0m\n\u001b[1;32m    345\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epoch: %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_fitted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/repo/projects/timeseries-generation/model/org_vrae.py\u001b[0m in \u001b[0;36m_train\u001b[0;34m(self, train_loader)\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m             \u001b[0;31m# Index first element of array to return tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_shared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__module__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'numpy'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'str_'\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'string_'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [100, 56] at entry 0 and [28, 56] at entry 15"
     ]
    }
   ],
   "source": [
    "vrae.fit(train_dataset)\n",
    "\n",
    "#If the model has to be saved, with the learnt parameters use:\n",
    "# vrae.fit(dataset, save = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform the input timeseries to encoded latent vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_run = vrae.transform(test_dataset)\n",
    "\n",
    "#If the latent vectors have to be saved, pass the parameter `save`\n",
    "# z_run = vrae.transform(dataset, save = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the model to be fetched later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vrae.save('vrae.pth')\n",
    "\n",
    "# To load a presaved model, execute:\n",
    "# vrae.load('vrae.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize using PCA and tSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_clustering(z_run, y_val, engine='matplotlib', download = False)\n",
    "\n",
    "# If plotly to be used as rendering engine, uncomment below line\n",
    "#plot_clustering(z_run, y_val, engine='plotly', download = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
